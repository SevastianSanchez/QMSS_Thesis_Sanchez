data = panel_data,
subset = paste(group), # modifying based on regime stability
effect = "twoways")
# baseline model
FE_no_spi <- plm(formula = sdg_overall ~ di_score + dplyr::lag(di_score, 1) + dplyr::lag(di_score, 2) + log_gdppc + income_level_recoded,
model = "within",
index = c("country_code", "year"),
data = panel_data,
subset = group, # modifying based on regime stability
effect = "twoways")
# baseline model
FE_no_spi <- plm(formula = sdg_overall ~ di_score + dplyr::lag(di_score, 1) + dplyr::lag(di_score, 2) + log_gdppc + income_level_recoded,
model = "within",
index = c("country_code", "year"),
data = panel_data,
subset = group, # modifying based on regime stability
effect = "twoways")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(plm)
library(ggplot2)
library(reshape2)
library(dplyr)
library(car)  # For VIF calculation
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R")
# Extract the data
corr_data <- all_data %>%
dplyr::select(sdg_overall, spi_comp, sci_overall, di_score, log_gdppc, income_level, population, country_name, country_code, year, goal1:goal17, p1_use, p2_services, p3_products, p4_sources, p5_infra)
library(tidyverse)
#calls sources
source("data/data_sources.R")
?WDI
WDIsearch()
?WDIsearch()
WDIsearch(string='pop', field='name')
WDIsearch(string='total population', field='name')
WDIsearch(string='1.1_ACCESS.ELECTRICITY.TOT', field='indicator')
WDIsearch(string='SP.POP.TOTL', field='indicator')
#
WDIsearch(string='Wittgenstein Projection: Percentage of the total population by highest level of educational attainment. Lower Secondary. Total', field='name')
WDIsearch(string='education', field='name')
WDIsearch(string='secondary education', field='name')
WDIsearch(string='CC.SE.CAT3.ZS', field='indicator')
#
#
WDIsearch(string='CC.SE.CAT3.ZS', field='indicator')
#
WDIsearch(string='CC.SE.CAT3.ZS', field='indicator')
WDIsearch(string='CC.SE.CAT4.ZS', field='indicator')
WDIsearch(string='tertiary education', field='name')
WDIsearch(string='population with tertiary education', field='name')
# % population with tertiary education
WDIsearch(string='total population with tertiary education', field='name')
WDIsearch(string='research and development', field='name')
WDIsearch(string='R&D', field='name')
WDIsearch(string='Gross National Income', field='name')
WDIsearch(string='GINI', field='name')
WDIsearch(string='gni classification', field='name')
WDIsearch(string='gni income', field='name')
WDIsearch(string='GNI classification', field='name')
WDIsearch(string='GNI level', field='name')
WDIsearch(string='development status', field='name')
WDIsearch(string='LIC', field='name')
WDIsearch(string='LMIC', field='name')
WDIsearch(string='country classification', field='name')
WDIsearch(string='developed', field='name')
WDI(country = "all", indicator = "3.0.Gini", start = 2000, end = NULL)
testing2.0 <- df_years2.0(2004, 2023)
#Set Directory
#setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez/data/misc")
#vdem = country_text_id, country_name, [COWcode]
#spi = iso3c
#sdg = country_code, country_name
#sci = country_code
#ert = country_text_id, country_name, country_id,
#gdppc = country_code
#infocap = country_id
#income class = country_code
#di = country_code
#gini = Code
country_colnames <- c(
"country", "country_code", "ccodecow", "COWcode", "cowcode", "iso3", "iso3c",
"country_name", "country_id", "countryname", "Entity", "Code", "country_text_id", "countryid"
)
# Define forward mapping (country names/aliases to ISO3 codes)
custom_match_forward <- c(
"Kosovo" = "XKX",
"Somaliland" = "SOL",
"Zanzibar" = "ZNZ",
"Channel Islands" = "CHI",
"Saint Martin (French part)" = "MAF",
"Curaçao" = "CUW",
"West Bank and Gaza" = "PSE",
"Congo (Kinshasa)" = "COD",
"Myanmar (Burma)" = "MMR",
"Iran, Islamic Rep." = "IRN",
"Kyrgyz Republic" = "KGZ",
"Bahamas" = "BHS",
"Belize" = "BLZ",
"Brunei" = "BRN",
"South Sudan" = "SSD"
)
# Define reverse mapping (ISO3 codes to country names)
custom_match_reverse <- c(
"XKX" = "Kosovo",
"SOL" = "Somaliland",
"ZNZ" = "Zanzibar",
"CHI" = "Channel Islands",
"MAF" = "Saint Martin (French part)",
"CUW" = "Curaçao",
"PSE" = "West Bank and Gaza",
"COD" = "Congo (Kinshasa)",
"MMR" = "Myanmar (Burma)",
"IRN" = "Iran, Islamic Rep.",
"KGZ" = "Kyrgyz Republic",
"BHS" = "Bahamas",
"BLZ" = "Belize",
"BRN" = "Brunei",
"SSD" = "South Sudan"
)
standardize_country_codes <- function(df) {
# Identify country columns
country_cols <- names(df)[tolower(names(df)) %in%
tolower(country_colnames)]
if(length(country_cols) > 0) {
for(col in country_cols) {
try({
# custom matching
df$iso3_standardized <- countrycode(
df[[col]],
origin = case_when(
tolower(col) == "iso2" ~ "iso2c",
tolower(col) == "iso3" ~ "iso3c",
tolower(col) == "country_code" ~ "iso3c",
TRUE ~ "country.name"
),
destination = "iso3c",
# Add custom matches here
custom_match = c(custom_match_forward
)
)
break
}, silent = TRUE)
}
}
if ("iso3_standardized" %in% names(df)) {
# Use countrycode's built-in mappings for standard ISO3 codes
df$country_name <- countrycode::countrycode(
sourcevar = df$iso3_standardized,
origin = "iso3c",
destination = "country.name",
# Only override for non-standard codes (e.g., Kosovo)
custom_match = custom_match_reverse
)
}
if(!"iso3_standardized" %in% names(df)) {
warning("No suitable country code column found in dataset")
}
return(df)
}
#Processing datasets for merging
process_datasets <- function(df_lists) {
merged_data <- list()
for(i in seq_along(df_lists)) {
df <- df_lists[[i]]
# Standardize year column naming
year_col <- names(df)[tolower(names(df)) %in% c("year")]
if (length(year_col) == 0) {
year_col <- names(df)[grepl("year", tolower(names(df)))]
}
if (length(year_col) > 0 && year_col != "year") {
names(df)[names(df) == year_col[1]] <- "year"
}
if ("year" %in% names(df)) {
df <- df %>%
mutate(year = suppressWarnings(as.integer(year))) %>%
filter(!is.na(year))
}
df <- standardize_country_codes(df)
# Identify original country columns
orig_cols <- names(df)[tolower(names(df)) %in% tolower(country_colnames)]
new_orig_names <- character(0)
if(length(orig_cols) > 0) {
new_orig_names <- paste0("orig_", orig_cols, "_ds", i)
names(df)[match(orig_cols, names(df))] <- new_orig_names
}
# Identify non-key columns for suffixing
#protected_cols <- c("iso3_standardized", "year", new_orig_names)
#non_key_cols <- setdiff(names(df), protected_cols)
#if(length(non_key_cols) > 0) {
#  new_names <- paste0(non_key_cols, "_ds", i)
#  names(df)[match(non_key_cols, names(df))] <- new_names
#}
# Skip datasets without required columns
if(!"iso3_standardized" %in% names(df)) {
warning(paste("Skipping dataset", i, "- no country codes found"))
next
}
if(!"year" %in% names(df)) {
warning(paste("Dataset", i, "missing year column"))
next
}
merged_data[[i]] <- df
}
# DEDUPLICATE HERE:
merged_data <- lapply(merged_data, function(df) {
if(all(c("iso3_standardized", "year") %in% names(df))) {
df %>%
group_by(iso3_standardized, year) %>%
slice(1) %>%
ungroup()
} else {
df
}
})
# Merge with reduced memory usage
final_df <- merged_data %>%
reduce(
function(x, y) {
full_join(x, y, by = c("iso3_standardized", "year"))
},
.init = tibble(iso3_standardized = character(), year = numeric())
)
return(final_df)
}
#importing data function
import_data <- function(year1, year2) {
source("years_filter.R")
new_list_df <- years_filter(start_yr = year1, end_yr = year2)
return(new_list_df)
}
#Loading Necessary Data
df_years2.0 <- function(x, y) {
list_for_dfs <- import_data(x, y)
processed <- process_datasets(list_for_dfs)
data <- processed %>%
dplyr::mutate(year_fct = as.factor(year)) %>%
dplyr::mutate(across(c(income_level, regime_type_2, regime_type_4, regime_type_10, income_spi, region_spi), as.factor),
across(c(sci_overall, sci_method, sci_periodicity, sci_source), as.numeric)) %>%
dplyr::select(orig_country_name_ds1, iso3_standardized, year, year_fct, income_level, sdg_overall,
spi_comp, sci_overall, di_score, di_reg_type_2, log_gdppc, log_pop, gini_score, everything()) %>%
rename(country_name = orig_country_name_ds1,
country_code = iso3_standardized)
return(data)
}
testing2.0 <- df_years2.0(2004, 2023)
View(testing2.0)
corr_data <- corr_data %>%
mutate(log_pop = log(population))
View(corr_data)
corr_data <- corr_data %>%
mutate(log_pop = log(population))
# Impute missing data: log_gdppc
corr_data_impute <- corr_data %>%
dplyr::filter(year >= 2016) %>%
mutate(
log_gdppc = ifelse(is.na(log_gdppc), median(log_gdppc, na.rm = TRUE), log_gdppc),
population = ifelse(is.na(population), median(population, na.rm = TRUE), population)
)
# Impute missing data: log_gdppc
corr_data_impute <- corr_data %>%
dplyr::filter(year >= 2016) %>%
mutate(
log_gdppc = ifelse(is.na(log_gdppc), median(log_gdppc, na.rm = TRUE), log_gdppc),
population = ifelse(is.na(population), median(population, na.rm = TRUE), population)
)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(plm)
library(ggplot2)
library(reshape2)
library(dplyr)
library(car)  # For VIF calculation
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R") %>%
rm(c(fd_data, panel_data)) # removing panel dataframes
# Load necessary libraries
library(plm)
library(ggplot2)
library(reshape2)
library(dplyr)
library(car)  # For VIF calculation
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R") %>%
rm(c(fd_data, panel_data)) # removing panel dataframes
# Load necessary libraries
library(plm)
library(ggplot2)
library(reshape2)
library(dplyr)
library(car)  # For VIF calculation
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R")
# Extract the data
corr_data <- all_data %>%
dplyr::select(sdg_overall, spi_comp, sci_overall, di_score, log_gdppc, income_level, population, country_name, country_code, year, goal1:goal17, p1_use, p2_services, p3_products, p4_sources, p5_infra)
# Recode income_level to a factor with labels
corr_data <- corr_data %>%
mutate(log_pop = log(population))
# Impute missing data: log_gdppc
corr_data_impute <- corr_data %>%
dplyr::filter(year >= 2016) %>%
mutate(
log_gdppc = ifelse(is.na(log_gdppc), median(log_gdppc, na.rm = TRUE), log_gdppc),
population = ifelse(is.na(population), median(population, na.rm = TRUE), population)
)
# Calculate the correlation matrix
correlation_matrix <- cor(corr_data_impute[, c("sdg_overall", "spi_comp", "sci_overall", "di_score", "log_gdppc", "population")], use = "pairwise.complete.obs")
# Convert the correlation matrix to a data frame for ggplot
melted_correlation <- melt(correlation_matrix)
# Visualize the correlation matrix using ggplot2
correlation_plot <- ggplot(data = melted_correlation, aes(x = Var2, y = Var1, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(
low = "orange", mid = "white", high = "steelblue", midpoint = 0,
limits = c(-1, 1), name = "Correlation"
) +
geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
theme_minimal() +
labs(title = "Main Variables (2016-2023)", subtitle = "Correlation Matrix",
x = "", y = "") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Show the correlation plot
correlation_plot
# Load necessary libraries
library(plm)
library(ggplot2)
library(reshape2)
library(dplyr)
library(car)  # For VIF calculation
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R")
# Extract the data
corr_data <- all_data %>%
dplyr::select(sdg_overall, spi_comp, sci_overall, di_score, log_gdppc, income_level, population, country_name, country_code, year, goal1:goal17, p1_use, p2_services, p3_products, p4_sources, p5_infra)
# Impute missing data: log_gdppc
corr_data_impute <- corr_data %>%
dplyr::filter(year >= 2016) %>%
mutate(
log_gdppc = ifelse(is.na(log_gdppc), median(log_gdppc, na.rm = TRUE), log_gdppc),
population = ifelse(is.na(population), median(population, na.rm = TRUE), population)
)
# Recode income_level to a factor with labels
corr_data_impute <- corr_data_impute %>%
mutate(log_pop = log(population))
# Calculate the correlation matrix
correlation_matrix <- cor(corr_data_impute[, c("sdg_overall", "spi_comp", "sci_overall", "di_score", "log_gdppc", "log_pop")], use = "pairwise.complete.obs")
# Convert the correlation matrix to a data frame for ggplot
melted_correlation <- melt(correlation_matrix)
# Visualize the correlation matrix using ggplot2
correlation_plot <- ggplot(data = melted_correlation, aes(x = Var2, y = Var1, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(
low = "orange", mid = "white", high = "steelblue", midpoint = 0,
limits = c(-1, 1), name = "Correlation"
) +
geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
theme_minimal() +
labs(title = "Main Variables (2016-2023)", subtitle = "Correlation Matrix",
x = "", y = "") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Show the correlation plot
correlation_plot
# save the plot
ggsave("figures/correlation_matrix_main_vars.png", plot = correlation_plot, width = 5, height = 4, dpi = 300)
# Calculate the correlation matrix
correlation_matrix <- cor(corr_data_impute[, c("sdg_overall", "spi_comp", "sci_overall", "di_score", "log_gdppc", "log_pop")], use = "pairwise.complete.obs")
# Convert the correlation matrix to a data frame for ggplot
melted_correlation <- melt(correlation_matrix)
# Visualize the correlation matrix using ggplot2
correlation_plot <- ggplot(data = melted_correlation, aes(x = Var2, y = Var1, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(
low = "orange", mid = "white", high = "steelblue", midpoint = 0,
limits = c(-1, 1), name = "Correlation"
) +
geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
theme_minimal() +
labs(title = "Main Variables (2016-2023)", subtitle = "Correlation Matrix",
x = "", y = "") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Show the correlation plot
correlation_plot
# save the plot
ggsave("figures/correlation_matrix_main_vars.png", plot = correlation_plot, width = 5, height = 4, dpi = 300)
# Calculate the correlation matrix
correlation_matrix <- cor(corr_data_impute[, c("sdg_overall", "spi_comp", "sci_overall", "di_score", "log_gdppc", "log_pop")], use = "pairwise.complete.obs")
# Convert the correlation matrix to a data frame for ggplot
melted_correlation <- melt(correlation_matrix)
# Visualize the correlation matrix using ggplot2
correlation_plot <- ggplot(data = melted_correlation, aes(x = Var2, y = Var1, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(
low = "orange", mid = "white", high = "steelblue", midpoint = 0,
limits = c(-1, 1), name = "Correlation"
) +
geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
theme_minimal() +
labs(title = "Main Variables (2016-2023)", subtitle = "Correlation Matrix",
x = "", y = "") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Show the correlation plot
correlation_plot
# save the plot
ggsave("figures/correlation_matrix_main_vars.png", plot = correlation_plot, width = 5, height = 4, dpi = 300)
# Extract the data for SDG disaggregated scores
sdg_data <- corr_data %>%
dplyr::filter(year >= 2005) %>%
dplyr::select(country_name, country_code, year, goal1:goal17)
# Calculate the correlation matrix for SDG disaggregated scores
correlation_matrix_sdg <- cor(sdg_data[, -c(1, 2, 3)], use = "pairwise.complete.obs")
# Convert matrix to df
melted_cor_sdg <- melt(correlation_matrix_sdg)
# build matrix using ggplot and change names from goal1, goal2, ... to goal 1, goal 2, ...
correlation_plot_sdg <- ggplot(data = melted_cor_sdg, aes(x = Var2, y = Var1, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(
low = "orange", mid = "white", high = "steelblue", midpoint = 0,
limits = c(-1, 1), name = "Correlation"
) +
geom_text(aes(label = round(value, 2)), color = "black", size = 2.5) +
theme_minimal() +
labs(title = "SDG Disaggregated Scores (2005-2023)",
x = "", y = "", subtitle = "Correlation Matrix") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Show the correlation plot for SDG disaggregated scores
correlation_plot_sdg
# save
ggsave("figures/correlation_matrix_sdg_disaggregated.png", plot = correlation_plot_sdg, width = 6, height = 5, dpi = 300)
View(sdg_data)
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
# Load all packages from the packages.R file
source("packages.R")
# Load the panel data script
source("Comp2_panel_wrangling.R")
# Table of all countries and select regime change variables: has_aut_ep, has_dem_ep, has_neither, autocratized, democratized, and stable, and total counts of autocratization and democratization episodes [DONE!!]
regime_change_status <- panel_data %>%
select(country_name, country_code, has_aut_ep, total_aut_ep, has_dem_ep, total_dem_ep, has_neither, autocratized, democratized, stable) %>%
distinct() %>% # Ensure unique rows for each country (these are unique because they're 0/1 for all rows of a given country) [rows = 167]
arrange(country_code)
# Display the table
print(regime_change_status)
# Convert the regime change status table to a stargazer table for better visualization
stargazer(regime_change_status,
type = "html",
summary = FALSE,
title = "Regime Change Status of Countries",
digits = 0,
out = "compiled_materials/results/stats/regime_change_status_by_country.html")
# Frequency table of countries based on has_aut_ep, has_dem_ep, has_neither, autocratized, democratized, and stable variables [DONE!!]
freq_regime_change_status <- regime_change_status %>%
summarise(
Total_Countries = n(),
Countries_with_Autocratization_Episodes = sum(as.numeric(as.character(has_aut_ep)), na.rm = TRUE),
Countries_with_Democratization_Episodes = sum(as.numeric(as.character(has_dem_ep)), na.rm = TRUE),
Countries_with_Neither = sum(as.numeric(as.character(has_neither)), na.rm = TRUE),
Countries_that_Autocratized = sum(as.numeric(as.character(autocratized)), na.rm = TRUE),
Countries_that_Democratized = sum(as.numeric(as.character(democratized)), na.rm = TRUE),
Stable_Countries = sum(as.numeric(as.character(stable)), na.rm = TRUE)
) %>%
# pivot the freq_table to long format for better visualization
pivot_longer(cols = everything(), names_to = "Category", values_to = "Count")
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")
#load libraries/packages
source("packages.R")
# load data
source("Comp2_panel_wrangling.R")
# Contemporaneous Effect: SPI ~ DI
ols_spi_di <- plm(
formula = spi_comp ~ di_score + log_gdppc + income_level_recoded + factor(year),
index = c("country_code", "year"),
model = "pooling",
data = panel_data)
summary(ols_spi_di, vcov = vcovHC(ols_spi_di, cluster = "group", type = "HC1"))
# Adding Lag1: SPI ~ DI
ols_spi_di_L1 <- plm(
formula = spi_comp ~ di_score + dplyr::lag(di_score, n=1) + log_gdppc + income_level_recoded + factor(year),
index = c("country_code", "year"),
model = "pooling",
data = panel_data)
summary(ols_spi_di_L1, vcov = vcovHC(ols_spi_di_L1, cluster = "group", type = "HC1"))
# Adding Lag2: SPI ~ DI
ols_spi_di_L2 <- plm(
formula = spi_comp ~ di_score + dplyr::lag(di_score, n=1) + dplyr::lag(di_score, n=2) + log_gdppc + income_level_recoded + factor(year),
index = c("country_code", "year"),
model = "pooling",
data = panel_data)
summary(ols_spi_di_L2, vcov = vcovHC(ols_spi_di_L2, cluster = "group", type = "HC1"))
